% Note to self: if tools::texi2pdf ever hangs, delete all the LaTeX files (excluding Rnw of course)!!!
\documentclass{article}

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\usepackage{textcomp}
\usepackage[hang, flushmargin]{footmisc} 
\usepackage[version = 3]{mhchem} 
\usepackage[nottoc, numbib]{tocbibind} % So refs section is included in TOC
\usepackage{hyperref} % Must be loaded as the last package
\usepackage{tabularx}
\usepackage{float}
\usepackage{fancyvrb}
\hypersetup{colorlinks}

% To try to reduce margins
\setlength{\oddsidemargin}{10pt}
\setlength{\evensidemargin}{10pt}
\setlength{\topmargin}{7pt}
\setlength{\textwidth}{6in}
% And more on that. . . this is a mess of mixed up code
\addtolength{\topmargin}{-.7in}
\addtolength{\textheight}{1.4in}


\title{A Crash Course in Practical Data Analysis}
\author{Sasha Hafner\footnote{\texttt{sasha@hafnerconsulting.com}}}
\begin{document}
\maketitle
\thispagestyle{empty}

\setlength{\parskip}{10pt plus 1pt minus 1pt}
\setlength{\parindent}{0pt}

\maketitle

\newpage

<<setup, include = FALSE, cache = FALSE>>= 
library(knitr)
options(useFancyQuotes = FALSE)
opts_chunk$set(error = TRUE, comment = "#", out.height = "4.0in", out.width = "4.5in", fig.height = 5, fig.width = 6.75, fig.align = "center", cache = TRUE)
options(tidy = TRUE, width = 70)
rm(list = ls())
@

\newpage

\tableofcontents

\newpage

\section{Orientation}
Data analysis is essential in engineering research and practice. 
But the topic is confusing to many, and inaccurate or completely incorrect conclusions, wasted effort, and miserable students are just too common.
This short book provides a concise introduction to data analysis from a very practical perspective.
In it I try to explain and demonstrate some core concepts of data analysis and also data entry, manipulation, and related activities.

Much of this book is on relatively practical ``nuts and bolts'' of data analysis--including structuring your data, making data analysis reproducible, and the importance of visualization through plots.
Where statistical models are covered, I focus on a single relatively simple set that can be used for many types of analyses: classical linear models.
This is by no means a comprehensive introduction to \textit{anything}, except maybe what I was thinking of over the days that I wrote it.
However, it does, I think, contain several useful nuggets that could serve you well as you proceed in your academic and professional career.
And if I manage to help only one of you better understand and better practice data analysis, well. . . that is a pretty crappy return on my investment!
I hope it helps at least a dozen readers.

There are no real prerequisites for the material presented in this book, although some coursework in introductory statistics and at least a little experience with spreadsheets and some programming language would be helpful.
The examples I present were carried out using R and LibreOffice Calc, but other tools would work as well.
If you find that even the you cannot understand the output from the statistical models used here, or that the concepts are too complicated, spend some time with a good book on statistics first and then come back.
I can recommend \citet{zarBiostatisticalAnalysis1999}.

Much of the material you will see in the following pages is based on my own opinion, so I think it is fair for you to ask: ``Who the hell are you?''
I am a scientist with training in biology and engineering, with quite a bit of practical experience in data analysis. 
Presently I work on problems in environmental engineering from a modeling and data analysis perspective through my consultancy\footnote{More details at \url{www.hafnerconsulting.com}.}.
I started working regularly with the R language and environment in 2007, and most weeks work with R daily.
I have written several packages in R.\footnote{Including two on CRAN: biogas, for data processing and more in biogas research, and monitoR, for automated identification of animal vocalizations. See \url{https://cran.r-project.org/web/packages/available_packages_by_name.html} for these, or \url{https://github.com/sashahafner} for others.}

For better or worse, I do not have a degree in statistics nor one in programming.
I am sure that contributes to some confusion on statistical theory here and there, but for now let's pretend it is an advantage: it means I will focus on practical statistics that are relatively accessible.

\section{Data analysis steps}
You can think of the process of turning laboratory measurements into informative and useful results as occuring in 5 steps:
\begin{enumerate}
  \item Data entry (manual) or data collection (automated)
  \item Data processing
  \item Data manipulation
  \item Data visualization
  \item Data analysis
\end{enumerate}

Spreadsheet programs are incredibly useful for data entry and data storage.
While text files are simpler, quickly checking and correcting data is easier with a spreadsheet.
In Section \ref{sec:organize} below you can find some advice on how to organize data you enter into a file.

I have used the term ``data processing'' to refer to transforming ``raw'' measurements into quantities and units that are useful, e.g., converting measured biogas volume in biochemical methane potential, or even transforming electrode potential measurements into dissolved oxygen.
And data manipulation is a broad term that could cover most of the steps listed above.
But here I mean changing the structure of your data--the way they are organized within data objects--in order to use them in the next two steps.
(The hip term for this process has become ``data wrangling''.)
These steps generally require more time and effort than those that follow.
If you think ``data science'' is cool, you had better enjoy these types of tasks!

Data visualization is the processing of plotting data, to literally look for patterns or differences.
Please just accept here at the start that it is essential.
The examples below should make this clear.

Finally, the last step, data analysis, includes hypothesis tests through application of statistical models, perhaps as well as more mundane calculation of summaries.

This book will cover all of these steps in at least a \textit{little} detail.

\section{Working with organized data}
\label{sec:organize}
To make your life easier and your research reproducible, the data you generate and work with should be well-organized.
This refers to both organization \textit{within} files, and organization \textit{of} files.
Data that are organized in a file in an umambiguous way are much more valuable than those that are not.
The former facilitate repeatable research, and can vastly extend the life of your measurements.
Of course, any electronic format is an improvement over data only stored on paper, but getting values into an electronic file is not in itself sufficient!

The following guidelines for organization were originally developed for data that will be analyzed using R or similar software (see Section \ref{sec:softwaretype}), but even if you plan to carry out all data analysis using spreadsheet software, they are still useful.

\begin{enumerate}
  \item Header rows are only present at the top of the file
  \item Each column contains a single variable
  \item Each row contains a single observation
  \item Each file (or worksheet) contains a single block of data
\end{enumerate}

This is probably best shown by example.
See the files silage\_comp\_original.xlsx and silage\_comp\_restruct.xlsx for an example.
Half of the original file is shown below in Fig. \ref{fig:silageorig}.
This file violates rules 1, 2, 3 (although it is not clear in Fig. \ref{fig:silageorig}, there is another set of block of data to the right), and 4.
This structure is pretty easy to understand and a person could interpret it without much trouble.
But it would be very difficult to read the data into e.g., R and work with them.

The restructured file, the contents of which are shown in Fig. \ref{fig:silagerestruct}, in constrast, would be easy to work with.
It follows all of the rules listed above. 
The only feature that is perhaps a bit odd is the use of multiple header rows.
This turns out to be a convenient approach, however.
The first two rows provide information for understanding the data, including units and more details on the analytes. 
These ``exta'' headers are simply skipped when reading that data into R.

\begin{figure}
\includegraphics[width = 140mm]{images/silage_file_orig.png}
\caption{An example of a poor data structure. Data are on composition of silage (fermented animal feed) from a factorial experiment.}
\label{fig:silageorig}
\end{figure}

\begin{figure}
\includegraphics[width = 140mm]{images/silage_file_restruct.png}
  \caption{A better way to structure a file containing the data shown in Fig. \ref{fig:silageorig}. This file was manually manipulated using a mouse and keyboard in a spreadsheet program.}
\label{fig:silagerestruct}
\end{figure}

Sometimes researchers have a inclination to avoid repetition in data files, and so find the value in column B in Fig. \ref{fig:silagerestruct} to be inappropriate.
Perhaps this has to do with a focus on data entry efficiency.
If you have this perspective, please try to get over it!
For data files, the goal isn't to produce something beautiful.

Data that don't follow these rules can of course be restructured (reshaped) using R, Python, etc.
``Manual'' restructing via cut-and-paste etc. in a spreadsheet, or even manipulation of a text file is always possible, although somewhat risky.
In some cases it is the only plausible option.

Organizing files themselves presents its own significant challenges.
There are so many ways to make a mess of file organization, I am struggling to provide clear guidance!
Try to use a consistent structure for your projects. 
Avoid accumulating numerous copies of a file.
If you worry about making changes that you will later want to undo, consider making a switch from working in spreadsheets to working with a programming language, and see Section \ref{sec:tracking}.
If you receive data from a collaborator or a public source, it is good practice to save a copy of the original, e.g., in a sub-directory (folder) named ``data/original''.

\section{Tracking yourself}
\label{sec:tracking}
I think it is a bit strange that people carry around a mobile phone that regularly tells some Google or Apple server where they are and possibly what they are doing, but I am completely in favor of tracking my every move when it comes to data analysis.
In fact, doing so it essential for \textit{reproducible research}.
Ideally, any conclusion you write in a paper or report should be based on an analysis that can be checked, repeated, and corrected.
Data processing and manipulation are both prone to problems in this area.

Working with scripts in a programming language immediately solves part of this problem for you.
Why?--because a script contains a detailed description of exactly what you did.
This is a major advantage of using a programming language for your work, instead of a spreadsheet.

\fbox{\begin{minipage}{40em}
  \textbf{What the \#@!*\% is a script?} \\
  A \textit{script} is just a text file with some programming code.
  When doing data analysis with a programming language, it is typical (I might say essential) to enter and save your commands in scripts, which can be run or modified later.
\end{minipage}}

For complex analyses, or at least after you have a little experience with a new programming language, I recommend going one step further, and tracking \textit{changes} to your scripts.
I like to use Git and GitHub for this, but there are alternatives.
These are especially useful if you collaborate with others on data analysis.

Spreadsheets do not facilite repeating or checking any of the steps listed above, but this problem is especially accute for manual data manipulation.
For calculations underlying data processing and analysis, sure, one could find the formulas underlying the calculations, and follow the cell references to check everything. 
But there are so many damn cells, each with its own formula!
Code, in contrast, is concise by its nature. 
(You can find a bit more discussion on this difference in Section \ref{sec:softwaretype}.)
If you insist on sticking with a spreadsheet for data analysis, what can you do?
In the least, you can keep a log of major changes within each spreadsheet, in a dedicated worksheet.
Here it can be helpful to include the date, file name, your name, in addition to a description of the changes.
This is helpful for files used for only data entry as well--if you delete or correct a cell, record that change!

\section{Uses and abuses of statistics}
The term ``statistics'' can mean a few things, but here I mean statistical methods and statistical models used for data analysis.
Here I am also focused on \textit{inferential} statistics, i.e., methods and models meant to make some kind of inference.
For example, you might like to know if your new sludge treatment method improves biogas yields.
In some cases statistics can help answer a question like this, and in others, they are worse than useless.
To help understand why, it can be useful to think about some basic concepts in inferential statistics (box 1).

\fbox{\begin{minipage}{40em}
  \textbf{1. Inferential statistics: the basic concept} \\
  Inferential statistical methods are based comparing a \textit{difference} in some variable to \textit{random error} in that variable.
  If the observed difference is large compared to the error, we can \textit{infer} that a true difference exists in the (real or hypothetical) larger population.
\end{minipage}}

The difference is the effect you are interested in.
Thinking about the sludge example, the difference would be the size of the difference in yield between your new treatment and a reference treatment (or no treatment). 
Typically, we use the \textit{mean} or average difference as our best estimate of the effect.
We might have measured these results:

<<echo=FALSE>>=
set.seed(124)
d <- data.frame(x = factor(rep(c('Reference', 'Supa-dupa treatment'), each = 5)), y = rnorm(10, mean = 200, sd = 30))
stripchart(y ~ x, data = d, vertical = TRUE, method = 'jitter', xlab = '', ylab = 'Yield (some units)', ylim = c(0, max(d$y)))
@

Our mean values in this case are \Sexpr{signif(mean(subset(d, x == 'Reference')$y), 3)} (reference) and \Sexpr{signif(mean(subset(d, x == 'Supa-dupa treatment')$y), 3)} (the new treatment).
So there is a clear difference, right?!
No! There is always a difference, and we might need inferential statistics to determine \textit{if a difference really means anything}.
In this case, we can immediately see that there is no meaningful difference without even applying a statistical model.
Why?
Because the size of the difference is small compared to the random error.
There are no statistics needed to tell us this, making this example the first case where statistics are \textbf{not} useful.

The ``population'' about which we make inferences can be challenging to think about.
For observational studies, where 
Ideally, we would like to design experiments that provide information about how a process would function elsewhere.
For example, if we actually found a 30\% increase in methane yield from the supa-dupa process, we might hope that the same process would provide a similar improvement if applied to any sludge.
But variation in sludge composition could make this unlikely, so expecting this response is risky.
Perhaps then we should consider only sludge with similar characteristics, and our hypothetical population is all secondary (waste activated) sludge from municipal wastewater treatment plants.
Perhaps we need to consider solids retention time also.

\fbox{\begin{minipage}{40em}
  \textbf{2. The population concept} \\
  Inferences made using statistics apply to some real or imaginary population of observational units, from which your experiment sampled.
\end{minipage}}


What is random error anyway?
It is the sum of all the sources of error in our measurements which we do not completely understand and cannot completely control.
In the plot above, the variation in the vertical position of the points is a representation of the random error.
We call it random because we cannot predict its value for any one particular observation (i.e., any single point in the plot above).
But we can estimate its magnitude from our measurements.
And if we assume it follows some kind of distribution, we can use this information in statistical tests.
It is typical to assume (effectively define) that the expected mean value of random error is zero, and to estimate the value of random error inhererent in our measurements from the observed error.

Systematic error is different.
It does not have a mean of zero, is generally repeatable, and it may be possible to assign it to a particular cause.
For example, technician effects may be systematic errors. 
Depending on the experimental design and the analysis approach, one particular source of error could be identified (or not identified) as either random or systematic?
Confused?
Maybe the examples below will help.

How are these concepts important?
In applying statistical models we commonly assume that individual observations are \textit{independent} (see Section \ref{sec:CLM}, for example). 
If we want to make an estimate of the magnitude of random error, which is needed for a hypothesis test, we need to be sure that the the appropriate random error is reflected in our measurements.

For example, every idiot knows that \textit{replication} is important in statistics.\footnote{Although it is not always required in a strict sense. For example, a repeated-measures design can be considered a two-factor analysis of variance without replication \citep{zarBiostatisticalAnalysis1999}.}
But it must be the right kind of replication.
Replicating measurements on one single experimental unit that received some treatment and one single unit that did not can give you a lot of data, but you would both tend to underestimate random error and assign a likely systematic difference to your treatment!\footnote{This is related to the concept of \textit{pseudoreplication} or false replication, which has been discussed extensively \citep{hurlbertPseudoreplicationDesignEcological1984}.}
A statistical model alone cannot tell you about the effect of the treatment in this case, and complex fancy-sounding approaches do not solve the problem (although many try, as numerous published papers show).
Basic principles are important!

In other cases, it may be acceptable to apply a stastical model, but unnecessary.
When replication is done appropriately, and a measured difference is much larger than the random error, there may not be a need to apply a statistical model.
This is common in engineering, where most experiments are designed (in contrast to observational) and treatments have large effects.

We should add one point to this discussion. 
Even when statistics are useful, they are not the complete solution.
The \textit{magnitude} or the difference is more important.
In fact, we can turn the basic idea of inferential statistics on its head for many types of experiments by considering this: what is the probability that any two physical, chemical, or biological treatments you might work with to improve some process have the \textit{exact} same effect on the process?
Essentially nill. 
It is the magnitude of the difference that is important, so we must remember to consider both the size of the difference and the evidence we have about whether it reflects a true difference between treatments etc.

Inferential statistics have relied heavily on \textit{p}-values, which are meant to reflect the probability of a ``Type I'' error, which is what happens when you conclude that there \textit{is} a difference when in fact there is not.
The use of \textit{p}-values and in particular, the use of an arbitrary cutoff for assessing ``statistical significance'' has been strongly criticized over the past few decades, and statisticians have proposed (and argued about) alternatives \citep{wassersteinMovingWorld052019}.\footnote{This paper, actually an editorial in a special issue on the topic, provides a very interesting summary of the problem and proposed solutions \citep{wassersteinMovingWorld052019}.}
Advice includes completely dropping the use of a fixed cutoff (commonly called $\alpha$) as well as the term ``statistically significant''.
Instead, we might consider reporting actual \textit{p}-values, using confidence intervals, and always considering the magnitude of any difference.

I'll end this section with one last reminder about the limitations of experimental work.
Even very clear differences observed in your laboratory may not work out the same way in other laboratories, or at pilot or full scale. 
Why not?
For starters, there are always differences, and particularly for biological processes or even biological substrates, many differences are difficult to measure or even observe.
I and some collaborators have recently looked into the reproducibility of kinetic results extracted from BMP tests in the lab.\footnote{You can find a short presentation on this topic here: \url{https://www.bioenergie-events.de/cmp/program/short-presentations}.}
In many cases, results from individual laboratories could be used to show ``highly significantly different'' conversion rates between two substrates, e.g., \textit{p}-values well below 0.001.
But these results did not carry over to other laboratories, some of which had similarly ``significant'' results, but for the \textit{opposite} difference (Fig. \ref{fig:kcomp})!
So be modest, be careful, and accept that your results just may be wrong.

\begin{figure}
\includegraphics[width = 90mm]{images/sub_comp.png}
  \caption{A demonstration of a complete lack of reproducibility in inferential statistics results among laboratories. Individual labs measured first-order rate constant for two substrates, and the ratio of these values is shown on the x axis. In many cases, differences (i.e., a ratio above or below 1) were ``statistically significant'' based on a \textit{t}-test, shown by \textit{p}-values below 0.05.}
\label{fig:kcomp}
\end{figure}

\section{Spreadsheets and programming languages}
\label{sec:softwaretype}
R is a programming language and a software environment for statistical computing (Fig. \ref{fig:rstudio1}).
It happens to be my favorite tool for data analysis, partially because it is really good, and partially because I happened to be introduced to it a decade or two ago.
Python is also quite popular.

\begin{figure}
\includegraphics[width = 160mm]{images/rstudio1.png}
  \caption{The RStudio IDE, which can be used for working with the R language. It is available for free for Windows, Mac, and Linux, and is the easiest way to get started with R. You can find more details here: \url{https://www.rstudio.com/products/rstudio/download/}. Personally I much prefer to use the text editor Neovim with the Nvim-R plugin (\url{https://github.com/jalvesaq/Nvim-R}).}
\label{fig:rstudio1}
\end{figure}

But I would guess that Microsoft Excel, a spreadsheet program, is more popular by orders of magnitude.\footnote{I don't know if anyone has tried to make an estimate.}
Why?
It is probably partially related to history, but one reason may be that it is very simple to \textit{start} using Excel.
Challenges come later.
In contrast, gettings started with a new programming language is not always easy.
Also, Excel and other spreadsheet programs are cell-based. 
Users actually see and can manually manipulate their data.
A programming language requires some ability to visualize data objects and their manipulation and relate these to symbolic variable names and commands.
This isn't trivial.

\fbox{\begin{minipage}{40em}
  \textbf{One problem with spreadsheets} \\
  Spreadsheet data analysis is cell-based.
  Your results are generally dependent on many formulas distributed among many cells.
\end{minipage}}


Although it takes a bit more effort to get started with a programming language, it takes very little time to start getting dividends.
You will complete tasks more quickly, probably with more accurate results, and automatically produce a detailed record of your analysis, which can be used again and again.

If you are a student trying to improve your data analysis skills and practice, what should you do?
In general, I encourage students to learn a programming language, and the concensus seems to be that either R or Python is a good choice.
Python and R interpreters (the software that actually reads your code and does what you tell it to do) can be downloaded and installed for free, and there is an immense population of free resources on both.

Matlab is popular in academic settings, where it may actually be required for some course work. 
But it is not free or open-source, and it would not be unfair to say it has been eclipsed by alternatives that are.

\section{Some questions for you}
\label{sec:questions}
Some of the issues introduced above should be considered whenever analyzing data.
I can think of a few other points that are important as well.
Before analyzing you data, make some plots, think about your experiment, and ask yourself a few questions.
The list of questions below is not explained in great detail here, but it is referred to in the examples.

\begin{enumerate}
  \item What is the unit of observation? What is the thing on which you made measurements? Is it appropriate? It should reflect the population that you are interested in.
  \item Do you have replication, and it is the right type? Are the observations independent, apart from whatever factor you would like to test? 
  \item Are there systematic errors present in your data that could affect the results? If all obserations have the same systematic error (e.g., you made the measurements instead of your more experienced colleague) there is generally no reason to expect an effect on a comparison. If systematic error is associated with the experimental factor, however, you have a problem that statistical models cannot (easily) solve.
  \item Is your experimental factor continuous or categorical by nature? The answer determines the way you should analyze your data and interpret results.
  \item What type of a relationship do you expect (or see) between your treatments and the response variable(s)? How can your approach to data analysis accommodate this? Are transformations needed? Polynomials?
\end{enumerate}

\section{Classical linear models}
\label{sec:CLM}
The only type of statistical method I will present in this book is linear regression, which we can refer to as ``classical models'', because you can do more than just simple or multiple linear regression with this tool.
I've selected this method because I think it is extremely flexible and useful, and because I use it very frequently.
In this short book, I don't have the option of including many different approaches, but another reason to focus on a single method is to use it to support what I think is an important piece of advice: avoid using statistical methods that you do not understand.
It is likely that many statistical analyses you will need to do can be done using classical linear models, and it is relatively easy to understanding and apply.

In R, several classical statistical models can be implemented using one function: \texttt{lm} (for linear model).
The \texttt{lm} function can be used for simple and multiple linear regression, analysis of variance (ANOVA), and analysis of covariance (ANCOVA). 
With data transformations and polynomials, it can easily handle (some) non-normal error distributions and non-linear responses.

The arguments for \texttt{lm} are

<<>>= 
args(lm) 
@

The first argument, \texttt{formula}, is where you specify the basic structure of the statistical model.
This approach is used in other R functions as well, such as \texttt{glm}, \texttt{gam}, and others.
Venables et al. has a useful list of example formulas--some examples are repeated below.
In these examples, the variables \texttt{x}, \texttt{y}, and \texttt{z} are continuous, and \texttt{A}, \texttt{B}, and \texttt{C} are factors.

\begin{tabbing}
\hspace*{4.5cm} \= \hspace{1cm} \= \\
\texttt{y \string~ x   } \>   Simple linear regression of y on x \\
\texttt{y \string~ x + z   } \>   Multiple regression of y on x and z \\
\texttt{y \string~ poly(x, 2)   } \>   Second order orthogonal polynomial regression \\
\texttt{y \string~ x + I(x\string^2)}   \>   Second order polynomial regression \\
\texttt{y \string~ A   } \>   Single factor ANOVA \\
\texttt{y \string~ A + B   } \>   Two-factor ANOVA \\
\texttt{y \string~ A + B + A:B   } \>   Two-factor ANOVA with interaction \\
\texttt{y \string~ A*B   } \>   Two-factor ANOVA with interaction \\
\texttt{y \string~ (A + B + C)\string^2   } \>   Three-factor ANOVA with all first-order interactions \\
\texttt{y \string~ (A + B + C)\string^2 - B:C  } \> As above but without B:C interaction \\
\texttt{y \string~ A + x   } \>   ANCOVA \\
\end{tabbing}

Of course all this applies to R, but formulas in Python are similar, and even for spreadsheet programs, the concepts still apply.

When you apply classical linear models, you should be aware of the assumptions that are employed every time model coefficients, \textit{p}-values, or confidence intervals are returned.

\begin{enumerate}
  \item Errors are normally distributed
  \item Variance is constant
  \item Observations are independent
\end{enumerate}

For linear regression in particular, there are two more assumptions.

\begin{enumerate}
  \item The actual relationship is linear
  \item Error in predictor variables is negligible
\end{enumerate}

Some of these assumptions can be evaluated before even entering data, but others can only be evaluated after a model has been fit, somewhat ironically.
Functions available for the R language make it very easy to evaluate assumptions, but even spreadsheets can be used for the task with a bit of effort.

\section{Example 1: Removal efficiency in treatment wetlands}
Let's, finally, work on an example.
Two treatment wetlands were created and used to compare the efficacy of wastewater treatment by two species of plants:\textit{Phragmites australis} and \textit{Cyperus papyrus} \citep{garcia-avilaTreatmentMunicipalWastewater2020}.
The data are in the csv file wetlands.csv (text file with comma separators).
I will use R to plot the data.

First, let's load a handy function for summarizing datasets.\footnote{You can download this function from \url{https://github.com/sashahafner/jumbled}.}

<<>>=
source("functions/dfsumm.R")
@

<<>>=
wl <- read.csv('data/wetlands.csv')
@

<<>>=
dfsumm(wl)
@

These data are in a structure midway between ``long''  and ``wide''.
I'll reshape it first, and for that I need an add-on package.
I'll load the graphics and date/time packages as well.

<<>>=
library(reshape2)
library(ggplot2)
library(lubridate)
@

I'll reshape these data in a couple ways.

<<>>=
wll <- melt(wl, id.vars = c('date', 'parameter', 'unit'), 
            measure.vars = c('influent', 'eff_cp', 'eff_pa'),
            value.name = 'value', variable.name = 'source')
ww <- dcast(wll, date + source ~ parameter, value.var = 'value')
@

And I'll get day of the year for plotting.

<<>>=
ww$date <- mdy(paste(ww$date, '2000'))
ww$doy <- yday(ww$date)
@

We can look at the data now.
Here are the first few rows.

<<>>=
head(ww)
@

So we have influent composition, and the composition of effluent for the wetland with \textit{Phragmitis} and the one with \textit{Cyperis}.
Let's assume we are interested in ammonia.
I'll plot ammonia concentrations over time.

<<>>=
ggplot(ww, aes(doy, NH3.N, colour = source)) +
  geom_line() +
  geom_point() +
  labs(x = 'Day of year', y = 'Total ammonia N conc. (mg/L)')
@

It it clear that both wetlands remove ammonia--effluent concentrations are always below influent around the same date.
But can we compare the two plant species?
Let's start thinking about the questions in Section \ref{sec:questions} before we answer that question.

\begin{enumerate}
  \item The unit of observation is a single sampling time (date) for a single wetland.
  \item Yes, we have replication: we have multiple measurements for each wetland. Is it the right kind though? Umm. . .
\end{enumerate}

Are observations independent?
No--all the blue observations are from a single wetland, for example.
Well, is the replication the right kind then? 
Probably not, but it depends on the question we want to answer!
The obvious question is: ``Does the ammonia removal efficiency of wetland planted with these two plant species differ \textit{in general}?''
But to answer this question using inferential statistics, we would need multiple replicated wetlands for each species!
So no, we do not have the right kind of replication.
If we proceeded to apply a statistical model here we would really be comparing these two particular wetlands.
Could we be sure that any difference is due to the plants and not some other difference between the two wetlands, e.g., retention time, some other plants, degree of aeration?
Maybe not.

Unfortunately this situation may be unavoidable when it is difficult or expensive to create or treat some unit of observation--think about pilot-scale reactors, for example!
We do have the option of assuming that the plants are the main cause of differences between these two wetlands, but such a decision should be supported.
This could be done by e.g., running the wetlands in parallel for some time before adding plants, to show there is no difference.
Or, evidence could also come from other studies that show small variability among replicate wetlands.
Regardless, this leap of faith should be explicitly described, if it is employed.

In this particular case, we do not have enough knowledge to know if it is appropriate, and we didn't invest the money and effort in collecting the data, so it is easy to simply say they cannot be used in this way.
Furthermore, we can see in the plot that there is no consistent difference--so there is clearly no reason to apply a statistical model here.
The only value I see in these data is in presenting the estimates of removal efficiency, but of course without any estimate of variability.
Let's estimate removal efficiency though.
For that it is easiest if we have the different wetlands in separate columns.

<<>>=
head(wl)
@

Let's focus on ammonia and a few other variables where removal efficiency makes sense.

<<>>=
levels(wl$parameter)
w2 <- subset(wl, parameter %in% c('TSS', 'BOD5', 'COD', 'NO3.N', 'NH3.N', 'TP'))
@

The hydraulic retention time in these wetlands is only 1 d, so it is not unreasonable to assume influent and effluent samples collected on the same day are related.

<<>>=
w2$reff_cp <- 100 * (1 - w2$eff_cp / w2$influent)
w2$reff_pa <- 100 * (1 - w2$eff_pa / w2$influent)
@

<<>>=
head(w2)
@

And we can calculate mean values and standard deviation.

<<>>=
w3 <- melt(w2, measure.vars = c('reff_cp', 'reff_pa'),
           value.name = 'reff', variable.name = 'wetland')

summarise(group_by(w3, parameter, wetland), reff_mean = mean(reff), reff_sd = sd(reff))
@

So there are some ``statistics''--average removal efficiency and some estimate of variability over time for these \textbf{two} wetlands.


\section{Example 2: Comparing two BMP methods}
I did some work on the evaluation of a new method for measurement of biochemical methane potential (BMP) a couple years ago \citep{justesenDevelopmentValidationLowcost2019}.
We were interested in determining if the method gave different results from other methods.
Let's get the data.

<<>>=
bb <- read.csv('data/BMP_comp.csv')
@

<<>>=
bb
@

The response variable is BMP measured for a single bottle using a particular method (gravimetric or the new one, GD-BMP, \texttt{gdt}). 
The unit of observation is a single bottle.
It looks like we have replication--three bottles for all substrates except cellulose, for which we have two.
Importantly, it seems that both of the methods were applied to each bottle (the \texttt{id} column has a unique bottle identifier).
If possible, this approach (paired measurements or repeated measures) is more \textit{powerful} than the use of separate units of observation, because some of the random error is eliminated.

Let's plot these data.

<<>>=
ggplot(bb, aes(substrate, bmp, colour = method)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(x = 'Substrate', y = 'BMP (mL/g)', colour = 'BMP')
@

So what do we see?
Is there evidence of differences between the methods?
Can we compare them?
There is in fact virtually no evidence that the methods differ; results overlap for all substrates except ethanol.
Should we conclude then that the two methods give identical results?
Hell no!
In fact, we should assume from the start that they do not.
These two methods are based on different principles and each almost certainly have their own biases.
So perhaps our question should be changed to ``How different are the two methods?''
The plot seems to suggest that the answer is ``not very, at least compared to measurement error''.

Something else we can see in this plot is that the variability of the GD method is higher than the gravimetric method.
Is this meaningful?
Well, here we see that for five different substrates, GD results were much more variable for all of them.
That is pretty strong evidence that the difference is real.
Later, perhaps, we can apply a statistical test.
But it is worth pointing out that this difference could be an impediment to applying statistical models for comparing BMP, because we would typically have to assume that variance is constant.

Thinking more about the difference between the methods, we should consider the paired nature of the measurements.
Let's calculate a difference between the two methods for each individual bottle. 
That will remove some of the random error associated with bottles, and, conveniently, eliminates the problem with unequal variance.
See how experimental design can help you?
In general, a paired approach is more powerful, that is, more likely to show a clear difference when in fact one exists.

<<>>=
library(tidyr)
@

<<>>=
bw <- spread(bb, method, bmp)
head(bw)
@

We can now easily calculate a difference for each bottle, and I'll add a relative difference (\% of gravimetric result) as well.

<<>>=
bw$diff <- bw$gdt - bw$grav
bw$rdiff <- 100 * bw$diff / bw$grav
@

<<>>=
ggplot(bw, aes(substrate, rdiff, colour = substrate)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(x = 'Substrate', y = 'Difference in BMP (%)', colour = 'BMP')
@

Here as well, there is no evidence of a consistent difference between the methods, possibly excluding ethanol.
We can still conclude then, without fitting a statistical model, that there is no significant evidence of a systematic difference between the methods.

But, maybe we would like to say how large a difference \textit{might} exist.
We could use these data for this, and can (finally) apply a statistical model!
Let's use the relative difference as the response variable, because it likely to be less variable than the absolute differences.
Still, we should include the fact that bottles with the same substrate are not independent.
We'll fit a classical linear model using the \texttt{lm()} function in R.
Because \texttt{substrate} is a factor (categorical variable) and not continuous, R will automatically create dummy variables for us.
Essentially we are carrying out analysis of variance (ANOVA) here.

<<>>=
mod1 <- lm(rdiff ~ substrate - 1, data = bw)
summary(mod1)
@

Not surprisingly, there is no evidence of a difference.
But how large could a difference be?
We can get confidence intervals to tell us this.

<<>>=
confint(mod1)
@

We can say then, that at the 95\% confidence, any difference between the methods is probably smaller than 17\%, with the exception of ethanol.
That is useful!
If we want more information or a better estimate of any probable difference between the methods, we would need to carry out additional experiments.

Could this analysis have been done using a spreadsheet?
Yes, but with some difficulty, and an unforunate lack of clarity and reproducibility.
In LibreOffice Calc, it is necessary to first calculate the response variable (relative difference), as in the R code above, but then to add columns with ``dummy variables'' for substrate.
This is shown in the figure below (Fig. \ref{fig:bmpspreadsheet1}), but also in the file ``BMP\_comp.ods'' in the spreadsheets directory.

\begin{figure}
\includegraphics[width = 150mm]{images/BMP_comp_spreadsheet.png}
\caption{BMP comparison data ready for analysis in LibreOffice Calc (a spreadsheet program).}
\label{fig:BMPspreadsheet1}
\end{figure}

To actually fit the regression model, the ``Data'' menu is selected, then ``Statistics'', and finally, ``Regression''.
Variables and options are selected as shown below (Fig. \ref{fig:bmpspreadsheetanalysis}).

\begin{figure}
\includegraphics[width = 70mm]{images/BMP_comp_spreadsheet_analysis.png}
\caption{Inputs required for analysis of the BMP data in LibreOffice Calc.}
\label{fig:BMPspreadsheetanalysis}
\end{figure}


\section{Example 3: VOC emission from silage}
The data in \texttt{ethanol\_emis.xlxs} are on ethanol emission from maize silage (fermented cattle feed) measured in a simple wind tunnel.
Emission was measured from 15 cm thick samples of silage taken from bunker silos, where silage is stored for weeks or months.
The measurements were part of a crossed factorial experiment designed for evaluating the effect of temperature and wind speed at the silage surface on emission rate.
The response variable is in the last column: \texttt{emis.n} (for emission, normalized), and is the fraction of initial ethanol mass lost over 12 hours of emission. 
Temperature and relative humidity were controlled using an environmental chamber.
Air speed was controlled using a blower and a system of valves.
The target value is given in \texttt{speed.tar} while the actual value is in \texttt{speed}.
Silage density was not controlled, but was determined because it affects porisity, which could affect emission rate.
Silage gas-phase porosity was determined from density and dry matter content.

The primary question we were interested in was how do temperature and air speed affect ethanol emission?

<<>>=
library(tidyverse)
library(readxl)
@

Read in the data.

<<>>=
et <- read_excel("data/ethanol_emis.xlsx", skip = 1)
@

Check data

<<>>=
dfsumm(et)
@

Everything looks OK.
No missing values.
All the variables we are interested in are numeric, except \texttt{box}, which is character.
We will need to convert it to a factor later.
Notice the ranges--\texttt{emis.n} is all between 0.016 and 0.501.

A plot is the best place to start.

<<>>=
ggplot(et, aes(speed, emis.n, colour = factor(temp.c))) + 
  geom_point() +
  labs(x = 'Air speed (m/s)', y = 'Normalized emission (frac. initial)', 
       colour = 'Temperature') +
  theme(legend.position = 'top')
@

Clearly air speed, and probably temperature, affected emission of ethanol.
But let's pause and think about a few other things here.

First, what kind of relationships should we expect?
Thinking about this will help us display the data in the best way, and later, fit the most appropriate model.
You can assume that ethanol emission is related to its volatility, which we can quantify using Henry's law constant.
How does volatility respond to temperature?
It sure isn't linear. 
In fact, a common assumption is that Henry's law constant changes by a factor of 2 with every 10$^\circ$C change in temperature.
The form of this statement suggests a logarithmic relationship, so we should log transform emission.
How about air speed? 
If we think about correlations for mass transfer coefficients, they are not linear.
Here also, we could benefit from a transformation.

<<>>=
ggplot(et, aes(log10(speed), log10(emis.n), colour = factor(temp.c))) + 
  geom_point() +
  labs(x = 'Log10 air speed (m/s)', y = 'Log10 normalized emission (frac. initial)', 
       colour = 'Temperature') +
  theme(legend.position = 'top')
@

<<>>=
mod1 <- lm(log10(emis.n) ~ temp.c + log10(speed), data = et)

summary(mod1)

et$pred1 <- 10^predict(mod1)
et$resid1 <- resid(mod1)
@

<<>>=
ggplot(et, aes(log10(speed), log10(emis.n), colour = factor(temp.c))) + 
  geom_point() +
  geom_point(aes(y = log10(pred1)), pch = 1) +
  geom_line(aes(y = log10(pred1))) +
  labs(x = 'Log10 air speed (m/s)', y = 'Log10 normalized emission (frac. initial)', 
       colour = 'Temperature') +
  theme(legend.position = 'top')
@

So we could end here.
But how about porosity?
It was not controlled, but we might expect that it could affect volatilization, because volatile compounds could travel through gas pores.
Wouldn't it be great if it explained much of the variability not clearly related to the other experimental factors?
Let's take a look.

<<>>=
ggplot(et, aes(por.g, log10(emis.n), colour = factor(temp.c))) + 
  geom_point() +
  facet_wrap(~ speed.tar) +
  theme(legend.position = 'top')
@

There is very clear correlation and we should include it!

<<>>=
mod2 <- lm(log10(emis.n) ~ temp.c + log10(speed) + por.g, data = et)

summary(mod2)

et$pred2 <- 10^predict(mod2)
et$resid2 <- resid(mod2)
@

<<>>=
ggplot(et, aes(log10(speed), log10(emis.n), colour = factor(temp.c))) + 
  geom_point() +
  geom_point(aes(y = log10(pred2)), pch = 1) +
  geom_line(aes(y = log10(pred2))) +
  labs(x = 'Log10 air speed (m/s)', y = 'Log10 normalized emission (frac. initial)', 
       colour = 'Temperature') +
  theme(legend.position = 'top')
@


<<>>=
ggplot(et, aes(log10(pred2), log10(emis.n), colour = factor(temp.c), 
               shape = factor(speed.tar))) + 
  geom_abline(intercept = 0, slope = 1) +
  geom_point() +
  labs(x = 'Log10 air speed (m/s)', y = 'Log10 normalized emission (frac. initial)', 
       colour = 'Temperature') +
  theme(legend.position = 'top')
@

\bibliography{bib}

\end{document}
